{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from splinter import Browser\n",
    "from splinter.exceptions import ElementDoesNotExist\n",
    "import time\n",
    "import re  #regular expression (from bs documentation)\n",
    "import selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://splinter.readthedocs.io/en/latest/drivers/chrome.html\n",
    "!which chromedriver\n",
    "\n",
    "executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "url = 'https://www.everydayhealth.com/drugs/truvada/reviews'\n",
    "#adding in splinter needed item\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = browser.html\n",
    "# Create BeautifulSoup object; parse with 'html.parser' \n",
    "#allows our code to parse through the items\n",
    "soup = bs(html, 'html.parser')\n",
    "#Retrieve all elements that contain organization information in an iterable list\n",
    "#results = soup.find('div', class_='star-rating-print').findChild()\n",
    "results = soup.body.find('div', class_='review-container row').findChildren()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test prints\n",
    "html = browser.html\n",
    "# results1 = soup.find('div',id='reviews-target')\n",
    "results = soup.body.find_all(class_='review-container row')\n",
    "for r in results:\n",
    "    whats = r.h3.text\n",
    "    print (whats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This only iterates through one page with the drug in the html already, now need to loop through all diff drug review pgs\n",
    "brands = []\n",
    "what_fors = []\n",
    "reviews = []\n",
    "dates_posted = []\n",
    "star_ratings = []\n",
    "\n",
    "#however when looking at the code h3 has a span within it that has text and that is the brand name\n",
    "#try for reasons to take\n",
    "for r in results:\n",
    "#if needed can add an if statement to say if r element has a__ then: ...\n",
    "#    brand_what = r.h3.text\n",
    "#    brand_what_strip = brand_what.strip(\"Rated\")\n",
    "    brand = r.h3.span.text\n",
    "#    print(brand)\n",
    "    brands.append(brand)\n",
    "    \n",
    "    #for reason (not sure yet how to only get the reason will work on later, maybe use if str.contains?)\n",
    "    #ok ?? using re (regex to remove what I dont want)https://www.regular-expressions.info/quickstart.html\n",
    "    #or do in df\n",
    "    whats = r.h3.text\n",
    "    print (whats)\n",
    "    what_fors.append(whats)\n",
    "\n",
    "    #need review which is in a p tag\n",
    "    review = r.p.text #its scraping the last one twice not sure why? ok not doing it in actual code...hmmmm\n",
    "#    print(review)\n",
    "    reviews.append(review)\n",
    "    \n",
    "    #need the date published, its in a span errrrr or really want the \n",
    "    #reference https://stackoverflow.com/questions/11205386/python-beautifulsoup-get-an-attribute-value-based-on-the-name-attribute\n",
    "    date = r.find('span', class_='time')['content']\n",
    "#    print (date)\n",
    "    dates_posted.append(date)\n",
    "    \n",
    "    #jumping to the stars rating, will figure out date after\n",
    "    rating = r.find('div', class_='star-rating-print').text.strip(\"Star\")\n",
    "#     print(rating)\n",
    "    star_ratings.append(rating)\n",
    "    \n",
    "    \n",
    "print(what_fors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_review_df = pd.DataFrame({'Name':brands,'Reason':what_fors,'Reviews':reviews,\n",
    "                               'Date Posted':dates_posted,'Ratings':star_ratings})\n",
    "drug_review_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/chromedriver\r\n"
     ]
    }
   ],
   "source": [
    "#https://splinter.readthedocs.io/en/latest/drivers/chrome.html\n",
    "!which chromedriver\n",
    "\n",
    "executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#46 brand name drugs\n",
    "all_brand_hiv_drugs = ['Combivir', 'Emtriva', 'Epivir', 'Epizicom','Hivid','Trizivir',\n",
    "    'Truvada','Videx','Viread','Zerit','Ziagen','Edurant','Intelence',\n",
    "    'Rescriptor','Pifeltro','Sustiva','Viramune','Agenerase','Aptivus',\n",
    "    'Crixivan','Invirase','Kaletra','Lexiva','Norvir','Prezista','Reyataz',\n",
    "    'Viracept','Isentress','Tivicay','Vitekta','Atripla','Biktarvy',\n",
    "    'Cimduo','Complera','Delstrigo','Dovato','Genvoya','Evotaz','Juluca',\n",
    "    'Odefsey','Prezcobix','Symfi','Stribild','Symtuza','Triumeq','Descovy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from urllib.request import Request, urlopen\n",
    "    #trying this Nope?\n",
    "#     req = Request(url , headers={'User-Agent': 'Mozilla/5.0'})\n",
    "#     webpage = urlopen(req).read()\n",
    "#     soup = bs(webpage, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "WebDriverException",
     "evalue": "Message: chrome not reachable\n  (Session info: chrome=79.0.3945.130)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5d32f14b3e5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#adding in splinter needed item\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mbrowser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbrowser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Create BeautifulSoup object; parse with 'html.parser'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/splinter/driver/webdriver/__init__.py\u001b[0m in \u001b[0;36mvisit\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvisit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0mLoads\u001b[0m \u001b[0ma\u001b[0m \u001b[0mweb\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mbrowser\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \"\"\"\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[1;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alert'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mWebDriverException\u001b[0m: Message: chrome not reachable\n  (Session info: chrome=79.0.3945.130)\n"
     ]
    }
   ],
   "source": [
    "#these will need to be w/n our loop & need a list of drugs (added above will need if statement if drug not found) \n",
    "drugs = ['atripla','truvada']\n",
    "for d in drugs:\n",
    "    # URL of page to be scraped\n",
    "    url = f'https://www.everydayhealth.com/drugs/{d}/reviews'\n",
    "    \n",
    "    #adding in splinter needed item\n",
    "    browser.visit(url)\n",
    "    html = browser.html\n",
    "    # Create BeautifulSoup object; parse with 'html.parser' \n",
    "    soup = bs(html, 'html.parser')\n",
    "    \n",
    "    time.sleep(10)\n",
    "    \n",
    "    #Retrieve all elements that contain organization information in an iterable list\n",
    "    if (soup.body.find(class_=\"error-404\")):\n",
    "        print(\"Drug Not Found\")\n",
    "    \n",
    "    else:\n",
    "        results = soup.body.find_all(class_='review-container row')\n",
    "        print(results)\n",
    "#ok this works now add to the info part of the scrape!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combivir: No Reviews\n",
      "Emtriva: No Reviews\n",
      "Epivir: No Reviews\n",
      "EpizicomDrug Not Found\n",
      "HividDrug Not Found\n",
      "Trizivir: No Reviews\n",
      "Truvada\n",
      "Truvada\n",
      "Truvada\n",
      "Videx: No Reviews\n",
      "Viread\n",
      "Zerit: No Reviews\n",
      "Ziagen: No Reviews\n",
      "Edurant: No Reviews\n",
      "Intelence: No Reviews\n",
      "Rescriptor: No Reviews\n",
      "Pifeltro: No Reviews\n",
      "Sustiva: No Reviews\n",
      "Viramune: No Reviews\n",
      "AgeneraseDrug Not Found\n",
      "Aptivus: No Reviews\n",
      "Crixivan: No Reviews\n",
      "Invirase: No Reviews\n",
      "Kaletra: No Reviews\n",
      "Lexiva: No Reviews\n",
      "Norvir: No Reviews\n",
      "Prezista: No Reviews\n",
      "Reyataz: No Reviews\n",
      "Viracept: No Reviews\n",
      "Isentress\n",
      "Tivicay\n",
      "Vitekta: No Reviews\n",
      "Atripla\n",
      "Atripla\n",
      "Atripla\n",
      "Atripla\n",
      "Atripla\n",
      "Atripla\n",
      "Atripla\n",
      "Atripla\n",
      "Atripla\n",
      "Atripla\n",
      "Atripla\n",
      "Atripla\n",
      "Biktarvy\n",
      "Cimduo: No Reviews\n",
      "Complera: No Reviews\n",
      "Delstrigo\n",
      "DovatoDrug Not Found\n",
      "Genvoya\n",
      "Genvoya\n",
      "Genvoya\n",
      "Genvoya\n",
      "Evotaz: No Reviews\n",
      "Juluca: No Reviews\n",
      "OdefseyDrug Not Found\n",
      "Prezcobix: No Reviews\n",
      "Symfi: No Reviews\n",
      "Stribild: No Reviews\n",
      "Symtuza\n",
      "Symtuza\n",
      "Symtuza\n",
      "Triumeq\n",
      "Triumeq\n",
      "Triumeq\n",
      "Triumeq\n",
      "Triumeq\n",
      "DescovyDrug Not Found\n",
      "['Truvada', 'Truvada', 'Truvada', 'Viread', 'Isentress', 'Tivicay', 'Atripla', 'Atripla', 'Atripla', 'Atripla', 'Atripla', 'Atripla', 'Atripla', 'Atripla', 'Atripla', 'Atripla', 'Atripla', 'Atripla', 'Biktarvy', 'Delstrigo', 'Genvoya', 'Genvoya', 'Genvoya', 'Genvoya', 'Symtuza', 'Symtuza', 'Symtuza', 'Triumeq', 'Triumeq', 'Triumeq', 'Triumeq', 'Triumeq']\n"
     ]
    }
   ],
   "source": [
    "#Keep lists outside of loops\n",
    "brands = []\n",
    "what_fors = []\n",
    "reviews = []\n",
    "dates_posted = []\n",
    "star_ratings = []\n",
    "\n",
    "for d in all_brand_hiv_drugs:\n",
    "    url = f'https://www.everydayhealth.com/drugs/{d}/reviews'\n",
    "    browser.visit(url)\n",
    "    html = browser.html\n",
    "    # Create BeautifulSoup object; parse with 'html.parser' \n",
    "    soup = bs(html, 'html.parser')\n",
    "    #Retrieve all elements that contain organization information in an iterable list\n",
    "    if (soup.body.find(class_=\"error-404\")):\n",
    "        print(f'{d}: Drug Not Found')\n",
    "    \n",
    "    elif (soup.body.find(class_=\"no-reviews\")):\n",
    "        print(f'{d}: No Reviews')\n",
    "    \n",
    "    else:\n",
    "        results = soup.body.find_all(class_='review-container row')\n",
    "#       print(results)\n",
    "\n",
    "        for r in results:\n",
    "            brand = d  #r.h3.span.text this would pull in generic name and some reviews dont include this\n",
    "            brands.append(brand)\n",
    "#             whats = r.h3.text\n",
    "#             what_fors.append(whats)\n",
    "#             review = r.p.text #its scraping the last one twice not sure why? ok not doing it in actual code...hmmmm\n",
    "#             reviews.append(review)\n",
    "#             date = r.find('span', class_='time')['content']\n",
    "#             dates_posted.append(date)\n",
    "#             rating = r.find('div', class_='star-rating-print').text\n",
    "#             star_ratings.append(rating)\n",
    "            print(brand)\n",
    "    #going to fast? getting a too many redirects clear cookies\n",
    "#     time.sleep(30)\n",
    "print(brands)\n",
    "\n",
    "#need to add loop through pages per drug if there are pages of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drugs.com Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://splinter.readthedocs.io/en/latest/drivers/chrome.html\n",
    "!which chromedriver\n",
    "\n",
    "#Initiate browser w/ Splinter\n",
    "executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped (so too many to type with, without brand and generic)\n",
    "url = 'https://www.drugs.com/comments/efavirenz-emtricitabine-tenofovir/atripla-for-hiv-infection.html'\n",
    "#adding in splinter needed item\n",
    "browser.visit(url)\n",
    "html = browser.html\n",
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = bs(html, 'html.parser')\n",
    "#Retrieve all elements that contain organization information in an iterable list\n",
    "results = soup.find('ul', class_='ddc-paging')\n",
    "lis = results.find_all('li')\n",
    "print((len(lis))-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiv_drugs2 = ['emtricitabine-tenofovir/truvada-','efavirenz-emtricitabine-tenofovir/atripla-',\n",
    "              'ritonavir/norvir-']\n",
    "              #,'raltegravir/isentress-','darunavir/prezista-',\n",
    "#               'cobicistat-elvitegravir-emtricitabine-tenofovir/stribild-',\n",
    "#               'abacavir-dolutegravir-lamivudine/triumeq- ','dolutegravir/tivicay- ',\n",
    "#               'emtricitabine-rilpivirine-tenofovir/complera-','abacavir-lamivudine/epzicom-',\n",
    "#               'cobicistat-elvitegravir-emtricitabine-tenofovir-alafenamide/genvoya-',\n",
    "#               'atazanavir/reyataz-','emtricitabine-tenofovir-alafenamide/descovy-',\n",
    "#               'lamivudine/','tenofovir/viread-','lamivudine/epivir-','lopinavir-ritonavir/kaletra-',\n",
    "#               'efavirenz/sustiva-','efavirenz-emtricitabine-tenofovir/','etravirine/intelence-','ritonavir/',\n",
    "#               'emtricitabine-tenofovir/','tenofovir/','abacavir/','bictegravir-emtricitabine-tenofovir-alafenamide/biktarvy-',\n",
    "#               'abacavir-lamivudine/','efavirenz/','raltegravir/','darunavir/','dolutegravir/','abacavir/ziagen-',\n",
    "#               'nevirapine/viramune-','atazanavir/','lamivudine-zidovudine/combivir-','cobicistat-darunavir/prezcobix-',\n",
    "#               'emtricitabine/emtriva-','fosamprenavir/lexiva-','emtricitabine-rilpivirine-tenofovir-alafenamide/odefsey-',\n",
    "#               'zidovudine/','nevirapine/','abacavirlamivudine-zidovudine/trizivir-','indinavir/crixivan-','emtricitabine/',\n",
    "#               'lamivudine/epivir-hbv','lopinavir-ritonavir/','stavudine/zerit-','abacavir-dolutegravir-lamivudine/',\n",
    "#               'didanosine/','emtricitabine-rilpivirine-tenofovir/','etravirine/','indinavir/','lamivudine-zidovudine/',\n",
    "#               'maraviroc/','stavudine/','didanosine/videx-','nelfinavir/viracept-','abacavir-lamivudine-zidovudine/',\n",
    "#               'tipranavir/aptivus-','cobicistat-elvitegravir-emtricitabine-tenofovir-alafenamide/',\n",
    "#               'atazanavir-cobicistat/evotaz-','raltegravir/isentress-hd-','dolutegravir-rilpivirine/juluca-',\n",
    "#               'bictegravir-emtricitabine-tenofovir-alafenamide/ ','cobicistat/','delavirdine/','doravirine/',\n",
    "#               'efavirenz-lamivudine-tenofovir/','emtricitabine-rilpivirine-tenofovir-alafenamide/',\n",
    "#               'emtricitabine-tenofovir-alafenamide/','enfuvirtide/','rilpivirine/',\n",
    "#               'cobicistat-darunavir-emtricitabine-tenofovir-alafenamide/symtuza-',\n",
    "#               'cobicistat-darunavir-emtricitabine-tenofovir-alafenamide/','dolutegravir-rilpivirine/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brands = []\n",
    "what_fors = []\n",
    "reviews = []\n",
    "dates_posted = []\n",
    "star_ratings = []\n",
    "\n",
    "for d in hiv_drugs2:\n",
    "    # URL of page to be scraped (so too many to type with, without brand and generic)\n",
    "    url = 'https://www.drugs.com/comments/{d}for-hiv-infection.html'\n",
    "    #adding in splinter needed item\n",
    "    browser.visit(url)\n",
    "    html = browser.html\n",
    "    # Create BeautifulSoup object; parse with 'html.parser'\n",
    "    soup = bs(html, 'html.parser')\n",
    "    #Retrieve all elements that contain organization information in an iterable list\n",
    "    results = soup.find('ul', class_='ddc-paging')\n",
    "    lis = results.find_all('li')\n",
    "    i = (len(lis))\n",
    "\n",
    "\n",
    "    for x in range(1,i):\n",
    "        #test prints\n",
    "        html = browser.html\n",
    "        results2 = soup.body.find('div', class_='contentBox')\n",
    "        brand = results2.h2.text #why is this stripping the last letter?\n",
    "        whats = \"HIV\"\n",
    "\n",
    "        results = soup.body.find_all('div', class_='ddc-comment')\n",
    "        for r in results:\n",
    "            #Get the review text\n",
    "            review = r.p.text \n",
    "            \n",
    "            #Get the date published\n",
    "            try:\n",
    "                date = r.find('span',class_='comment-date').text\n",
    "            except:\n",
    "                date = 'n/a'\n",
    "            \n",
    "            #Get the rating divide by 2 because it is a rating of 1-10 and we only want a rating 1-5\n",
    "            try:\n",
    "                rating = r.find('div', class_='rating-score').text #will have to divide by 2\n",
    "            except:\n",
    "                rating = 'n/a'\n",
    "    \n",
    "    \n",
    "            brands.append(brand)\n",
    "            what_fors.append(whats)\n",
    "            reviews.append(review)\n",
    "            dates_posted.append(date)\n",
    "            star_ratings.append(rating)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
